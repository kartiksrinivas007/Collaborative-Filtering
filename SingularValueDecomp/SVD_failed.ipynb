{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GGqQp7uJVzwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        userId  movieId  rating   timestamp\n",
            "0            1        1     4.0   964982703\n",
            "1            1        3     4.0   964981247\n",
            "2            1        6     4.0   964982224\n",
            "3            1       47     5.0   964983815\n",
            "4            1       50     5.0   964982931\n",
            "...        ...      ...     ...         ...\n",
            "100831     610   166534     4.0  1493848402\n",
            "100832     610   168248     5.0  1493850091\n",
            "100833     610   168250     5.0  1494273047\n",
            "100834     610   168252     5.0  1493846352\n",
            "100835     610   170875     3.0  1493846415\n",
            "\n",
            "[100836 rows x 4 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n",
            "(100837, 4)\n",
            "[[1.00000e+00 1.00000e+00 4.00000e+00]\n",
            " [1.00000e+00 3.00000e+00 4.00000e+00]\n",
            " [1.00000e+00 6.00000e+00 4.00000e+00]\n",
            " ...\n",
            " [6.10000e+02 1.68250e+05 5.00000e+00]\n",
            " [6.10000e+02 1.68252e+05 5.00000e+00]\n",
            " [6.10000e+02 1.70875e+05 3.00000e+00]]\n",
            "9742\n",
            "9742\n",
            "<class 'dict'>\n",
            "(9742,)\n",
            "201\n",
            "(array([   0,    2,    5,   43,   46,   62,   89,   97,  124,  130,  136,\n",
            "        184,  190,  197,  201,  224,  257,  275,  291,  307,  314,  320,\n",
            "        325,  367,  384,  398,  418,  436,  461,  476,  484,  485,  508,\n",
            "        509,  510,  513,  520,  546,  551,  559,  592,  594,  615,  632,\n",
            "        701,  705,  720,  723,  734,  767,  781,  782,  783,  786,  787,\n",
            "        788,  789,  797,  801,  810,  815,  820,  828,  829,  831,  836,\n",
            "        856,  863,  898,  899,  900,  907,  909,  911,  914,  915,  920,\n",
            "        921,  923,  925,  927,  939,  955,  957,  964,  969,  974,  977,\n",
            "        981,  990,  996, 1036, 1060, 1076, 1084, 1110, 1126, 1146, 1154,\n",
            "       1171, 1181, 1183, 1190, 1218, 1220, 1224, 1235, 1261, 1298, 1319,\n",
            "       1326, 1332, 1333, 1401, 1407, 1431, 1444, 1475, 1480, 1487, 1493,\n",
            "       1503, 1505, 1516, 1517, 1522, 1526, 1543, 1553, 1556, 1557, 1559,\n",
            "       1562, 1567, 1576, 1577, 1595, 1597, 1599, 1601, 1617, 1628, 1644,\n",
            "       1674, 1687, 1691, 1704, 1734, 1743, 1755, 1768, 1788, 1790, 1796,\n",
            "       1806, 1814, 1826, 1842, 1850, 1858, 1866, 1874, 1883, 1904, 1905,\n",
            "       1917, 1939, 1946, 1957, 1971, 1972, 1979, 1986, 1987, 1990, 1991,\n",
            "       1994, 1997, 2020, 2028, 2038, 2077, 2103, 2126, 2145, 2157, 2182,\n",
            "       2193, 2216, 2218, 2219, 2220, 2226, 2248, 2250, 2254, 2256, 2259,\n",
            "       2286, 2287, 2302, 2303, 2310, 2372, 2388, 2394, 2431, 2434, 2440,\n",
            "       2460, 2526, 2571, 2572, 2573, 2579, 2581, 2603, 2608, 2636, 2674,\n",
            "       2696, 2713, 2733, 2764, 2765, 2788, 2798, 2802, 2836, 2847, 2991,\n",
            "       3673]),)\n",
            "(611, 9742)\n",
            "(610, 9742)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "%run dataloader.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9742, 1)\n",
            "shape of rat_bool =  (9742, 610)\n",
            "shape of Rat =  (610, 9742)\n",
            "num_users = 610\n",
            "rat_tensor shape =  torch.Size([9742, 610])\n",
            "rat_tensor_masked shape =  torch.Size([9742, 610])\n",
            "sum shape =  torch.Size([9742, 1])\n",
            "tensor([[ 0.0791, -3.9209, -3.9209,  ..., -1.4209, -0.9209,  1.0791],\n",
            "        [-3.4318, -3.4318, -3.4318,  ..., -1.4318, -3.4318, -3.4318],\n",
            "        [ 0.7404, -3.2596, -3.2596,  ..., -1.2596, -3.2596, -3.2596],\n",
            "        ...,\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-4.0000, -4.0000, -4.0000,  ..., -4.0000, -4.0000, -4.0000]],\n",
            "       dtype=torch.float64)\n",
            "torch.Size([9742, 610])\n"
          ]
        }
      ],
      "source": [
        "Rat = final_ratings\n",
        "rat_bool = np.array(final_ratings.T > 0)\n",
        "nums = np.sum(rat_bool,axis = 1,keepdims=True)\n",
        "print(nums.shape)\n",
        "nums_tensor = torch.from_numpy(nums)\n",
        "print(\"shape of rat_bool = \",rat_bool.shape)\n",
        "rat_bool_tensor = torch.from_numpy(rat_bool)\n",
        "print(\"shape of Rat = \",Rat.shape)\n",
        "num_users = Rat.shape[0]\n",
        "print(\"num_users =\",num_users)\n",
        "num_movies = Rat.shape[1]\n",
        "num_features = 10\n",
        "rat_tensor = torch.from_numpy(Rat.T)\n",
        "print(\"rat_tensor shape = \",rat_tensor.shape)\n",
        "rat_tensor_masked = torch.mul(rat_tensor,rat_bool_tensor)\n",
        "print(\"rat_tensor_masked shape = \",rat_tensor_masked.shape)\n",
        "print(\"sum shape = \",torch.sum(rat_tensor_masked,dim=1,keepdim=True).shape)\n",
        "rat_tensor_mean = rat_tensor_masked - (torch.div( (torch.sum(rat_tensor_masked,dim=1,keepdim=True)) ,nums_tensor))\n",
        "print(rat_tensor_mean)\n",
        "X = torch.randn((num_movies,10),dtype=float,requires_grad = True)\n",
        "theta = torch.randn((num_features,num_users),dtype=float,requires_grad=True)\n",
        "preds = X @ theta\n",
        "\n",
        "print(rat_bool_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSjCagd7V055",
        "outputId": "4fe11c19-b468-489a-e8f6-a22082299e28"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Rat = np.array([[4,5,6,7,8,9],[3,4,6,7,1,9],[2,5,6,8,9,10]],dtype=float).T\n",
        "# Rat = torch.from_numpy(Rat)\n",
        "# print(Rat) # 5,3\n",
        "# X = torch.randn((6,4),dtype = float,requires_grad = True)\n",
        "# theta = torch.randn((4,3),dtype =float, requires_grad = True)\n",
        "# preds = X @ theta \n",
        "# loss_fn = F.mse_loss\n",
        "# weight_decay = 1e-5\n",
        "# regex = (torch.sum(torch.square(X)))*weight_decay\n",
        "# regtheta = (torch.sum(torch.square(theta)))*weight_decay\n",
        "# print(regex)\n",
        "# loss = loss_fn((X @ theta), Rat) + regex + regtheta\n",
        "# print(\"loss = \",loss) # use loss.item()\n",
        "# print(X,\"\\n\",theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2gP3-NWbwmD",
        "outputId": "2d8272ce-4d24-4b86-e7db-75b5a93d7c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[ 0.0791, -3.9209, -3.9209,  ..., -1.4209, -0.9209,  1.0791],\n",
            "        [-3.4318, -3.4318, -3.4318,  ..., -1.4318, -3.4318, -3.4318],\n",
            "        [ 0.7404, -3.2596, -3.2596,  ..., -1.2596, -3.2596, -3.2596],\n",
            "        ...,\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-4.0000, -4.0000, -4.0000,  ..., -4.0000, -4.0000, -4.0000]],\n",
            "       dtype=torch.float64),)\n",
            "[tensor([[ 0.0791, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,  0.5791, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -1.4209, -3.9209,\n",
            "          0.5791, -0.4209,  0.0791, -3.9209, -0.4209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -0.9209, -3.9209, -3.9209, -3.9209,  1.0791, -0.9209,\n",
            "         -0.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  1.0791, -0.9209,  0.0791,  1.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209, -3.9209, -3.9209, -0.9209, -3.9209, -3.9209,\n",
            "          1.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,  0.0791,\n",
            "         -3.9209,  0.0791, -3.9209, -1.4209, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "          0.5791, -3.9209, -3.9209, -3.4209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -1.4209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -0.9209, -0.9209,  0.0791, -3.9209, -0.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209,  0.5791, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209, -0.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -0.4209, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209, -1.9209, -3.9209, -0.9209,  0.0791, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209, -0.9209,  0.0791, -3.9209, -3.9209, -0.4209,\n",
            "          1.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "         -1.9209, -3.9209, -0.9209,  0.0791, -3.9209, -3.9209,  0.5791,  0.0791,\n",
            "          0.0791, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791, -0.4209, -3.9209,\n",
            "          0.5791, -3.9209,  1.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "          1.0791,  0.0791,  0.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "          0.0791,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,\n",
            "         -1.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -0.4209,\n",
            "          1.0791,  0.0791, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -0.4209, -0.9209, -3.9209, -0.9209,\n",
            "          0.0791, -3.9209, -0.4209,  1.0791, -3.9209, -3.9209, -0.4209, -3.9209,\n",
            "         -3.9209, -0.4209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209, -0.4209,\n",
            "         -0.9209,  1.0791, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791,  1.0791,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209,  0.5791, -3.9209,  0.5791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791,  0.0791,\n",
            "         -3.9209, -1.9209, -3.9209, -3.9209,  1.0791,  1.0791, -3.9209, -3.9209,\n",
            "          1.0791,  0.0791,  1.0791,  0.0791,  0.0791, -3.9209, -0.9209,  0.5791,\n",
            "         -3.9209,  0.5791, -0.9209, -3.9209, -3.9209, -3.9209, -3.9209,  0.5791,\n",
            "         -3.9209,  0.0791,  0.0791,  0.0791, -0.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -1.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.4209, -0.4209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791, -3.9209, -0.4209, -3.9209,  0.0791,\n",
            "          0.0791, -3.9209,  0.0791, -3.9209,  1.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "          1.0791, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209,  0.0791, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -0.9209, -0.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209,  0.5791, -3.9209,  1.0791, -0.4209,  0.5791, -3.9209, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -0.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  0.0791, -3.9209,\n",
            "         -0.4209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  1.0791, -1.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -1.4209,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791, -3.9209,  0.5791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  1.0791, -3.9209,  1.0791, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209, -3.9209,  0.5791, -3.9209, -2.4209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791,  0.0791,  0.0791,  1.0791, -3.9209,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791,  0.0791, -3.9209, -3.9209, -0.9209,\n",
            "         -3.9209, -3.9209,  0.0791,  0.5791, -3.9209, -3.9209, -3.9209,  0.5791,\n",
            "         -3.9209, -0.4209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,  0.0791,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209,  0.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209,  0.0791,  0.0791, -3.9209, -3.9209, -1.4209,\n",
            "         -0.9209, -3.9209, -3.9209, -3.9209,  1.0791,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -0.9209, -3.9209, -3.9209, -0.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,  1.0791, -0.9209,\n",
            "          0.0791,  0.5791, -3.9209, -3.9209, -3.9209, -3.9209, -0.4209, -3.9209,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791,  1.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -0.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791,  0.0791, -3.9209, -0.9209, -1.4209,\n",
            "          0.0791, -3.9209,  0.0791, -0.9209,  0.0791, -1.4209,  0.0791, -1.4209,\n",
            "         -0.9209,  1.0791]], dtype=torch.float64)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_ds = TensorDataset(rat_tensor_mean)\n",
        "print(train_ds.tensors)\n",
        "train_dl = DataLoader(train_ds,shuffle = False)\n",
        "for rat in train_dl: \n",
        "    print(rat)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model(X,theta):\n",
        "    return X @ theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(preds,targets,X,theta):\n",
        "    weight_decay = 1e-5\n",
        "    regex = (torch.sum(torch.square(X)))*weight_decay\n",
        "    regtheta = (torch.sum(torch.square(theta)))*weight_decay\n",
        "    return  (torch.square(torch.masked_select(rat_tensor_mean - (X @ theta),rat_bool_tensor))).sum() + regex + regtheta\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1086297.0241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor([-1548.7674, -1715.9091, -1818.8654,  ..., -2131.5000, -2131.5000,\n",
            "        -2436.0000], dtype=torch.float64)\n",
            "tensor([[-0.2442, -1.9849, -0.1376,  ..., -0.2817,  0.4772, -0.1127],\n",
            "        [-0.5512,  0.3451, -0.4545,  ...,  0.2815, -0.0739,  0.4858],\n",
            "        [ 0.4964, -0.5047, -0.0205,  ...,  0.7145,  0.1853,  1.5187],\n",
            "        ...,\n",
            "        [-0.2635, -1.0343, -0.1766,  ...,  1.5549,  0.6744, -0.4574],\n",
            "        [ 0.6654,  1.4743, -0.0398,  ..., -0.9085, -2.5979, -0.9922],\n",
            "        [ 0.7527, -0.4602, -0.4651,  ...,  0.2984, -1.5107, -2.3271]],\n",
            "       dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "preds = model(X,theta)\n",
        "print(loss_fn(preds,rat_tensor_mean,X,theta))\n",
        "print(torch.sum(rat_tensor_mean,dim = 1))\n",
        "loss = (loss_fn(preds,rat_tensor_mean,X,theta))\n",
        "loss.backward()\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/100: Loss: 1085604.7561712437\n",
            "Epoch 1/100: Loss: 1085603.6207766375\n",
            "Epoch 2/100: Loss: 1085602.486581551\n",
            "Epoch 3/100: Loss: 1085601.3535859727\n",
            "Epoch 4/100: Loss: 1085600.221789892\n",
            "Epoch 5/100: Loss: 1085599.0911932974\n",
            "Epoch 6/100: Loss: 1085597.961796177\n",
            "Epoch 7/100: Loss: 1085596.8335985206\n",
            "Epoch 8/100: Loss: 1085595.7066003156\n",
            "Epoch 9/100: Loss: 1085594.580801551\n",
            "Epoch 10/100: Loss: 1085593.4562022164\n",
            "Epoch 11/100: Loss: 1085592.3328022994\n",
            "Epoch 12/100: Loss: 1085591.2106017892\n",
            "Epoch 13/100: Loss: 1085590.0896006743\n",
            "Epoch 14/100: Loss: 1085588.9697989437\n",
            "Epoch 15/100: Loss: 1085587.8511965862\n",
            "Epoch 16/100: Loss: 1085586.7337935898\n",
            "Epoch 17/100: Loss: 1085585.617589944\n",
            "Epoch 18/100: Loss: 1085584.5025856374\n",
            "Epoch 19/100: Loss: 1085583.3887806586\n",
            "Epoch 20/100: Loss: 1085582.276174996\n",
            "Epoch 21/100: Loss: 1085581.164768639\n",
            "Epoch 22/100: Loss: 1085580.054561576\n",
            "Epoch 23/100: Loss: 1085578.9455537961\n",
            "Epoch 24/100: Loss: 1085577.8377452877\n",
            "Epoch 25/100: Loss: 1085576.7311360394\n",
            "Epoch 26/100: Loss: 1085575.6257260407\n",
            "Epoch 27/100: Loss: 1085574.5215152798\n",
            "Epoch 28/100: Loss: 1085573.4185037457\n",
            "Epoch 29/100: Loss: 1085572.316691427\n",
            "Epoch 30/100: Loss: 1085571.2160783126\n",
            "Epoch 31/100: Loss: 1085570.1166643915\n",
            "Epoch 32/100: Loss: 1085569.0184496527\n",
            "Epoch 33/100: Loss: 1085567.9214340844\n",
            "Epoch 34/100: Loss: 1085566.8256176761\n",
            "Epoch 35/100: Loss: 1085565.7310004162\n",
            "Epoch 36/100: Loss: 1085564.6375822932\n",
            "Epoch 37/100: Loss: 1085563.5453632965\n",
            "Epoch 38/100: Loss: 1085562.454343415\n",
            "Epoch 39/100: Loss: 1085561.3645226376\n",
            "Epoch 40/100: Loss: 1085560.2759009528\n",
            "Epoch 41/100: Loss: 1085559.1884783495\n",
            "Epoch 42/100: Loss: 1085558.1022548166\n",
            "Epoch 43/100: Loss: 1085557.0172303435\n",
            "Epoch 44/100: Loss: 1085555.9334049185\n",
            "Epoch 45/100: Loss: 1085554.8507785301\n",
            "Epoch 46/100: Loss: 1085553.7693511685\n",
            "Epoch 47/100: Loss: 1085552.6891228214\n",
            "Epoch 48/100: Loss: 1085551.6100934783\n",
            "Epoch 49/100: Loss: 1085550.5322631283\n",
            "Epoch 50/100: Loss: 1085549.45563176\n",
            "Epoch 51/100: Loss: 1085548.3801993614\n",
            "Epoch 52/100: Loss: 1085547.305965923\n",
            "Epoch 53/100: Loss: 1085546.232931433\n",
            "Epoch 54/100: Loss: 1085545.1610958804\n",
            "Epoch 55/100: Loss: 1085544.0904592548\n",
            "Epoch 56/100: Loss: 1085543.0210215438\n",
            "Epoch 57/100: Loss: 1085541.9527827373\n",
            "Epoch 58/100: Loss: 1085540.885742824\n",
            "Epoch 59/100: Loss: 1085539.819901793\n",
            "Epoch 60/100: Loss: 1085538.7552596333\n",
            "Epoch 61/100: Loss: 1085537.6918163334\n",
            "Epoch 62/100: Loss: 1085536.6295718832\n",
            "Epoch 63/100: Loss: 1085535.5685262708\n",
            "Epoch 64/100: Loss: 1085534.5086794859\n",
            "Epoch 65/100: Loss: 1085533.4500315168\n",
            "Epoch 66/100: Loss: 1085532.392582353\n",
            "Epoch 67/100: Loss: 1085531.336331984\n",
            "Epoch 68/100: Loss: 1085530.2812803977\n",
            "Epoch 69/100: Loss: 1085529.2274275837\n",
            "Epoch 70/100: Loss: 1085528.1747735306\n",
            "Epoch 71/100: Loss: 1085527.1233182284\n",
            "Epoch 72/100: Loss: 1085526.0730616653\n",
            "Epoch 73/100: Loss: 1085525.0240038307\n",
            "Epoch 74/100: Loss: 1085523.9761447138\n",
            "Epoch 75/100: Loss: 1085522.929484303\n",
            "Epoch 76/100: Loss: 1085521.8840225884\n",
            "Epoch 77/100: Loss: 1085520.8397595577\n",
            "Epoch 78/100: Loss: 1085519.7966952012\n",
            "Epoch 79/100: Loss: 1085518.7548295076\n",
            "Epoch 80/100: Loss: 1085517.7141624659\n",
            "Epoch 81/100: Loss: 1085516.6746940652\n",
            "Epoch 82/100: Loss: 1085515.6364242947\n",
            "Epoch 83/100: Loss: 1085514.5993531432\n",
            "Epoch 84/100: Loss: 1085513.5634806005\n",
            "Epoch 85/100: Loss: 1085512.5288066547\n",
            "Epoch 86/100: Loss: 1085511.495331296\n",
            "Epoch 87/100: Loss: 1085510.4630545129\n",
            "Epoch 88/100: Loss: 1085509.4319762946\n",
            "Epoch 89/100: Loss: 1085508.4020966303\n",
            "Epoch 90/100: Loss: 1085507.373415509\n",
            "Epoch 91/100: Loss: 1085506.3459329202\n",
            "Epoch 92/100: Loss: 1085505.3196488528\n",
            "Epoch 93/100: Loss: 1085504.2945632963\n",
            "Epoch 94/100: Loss: 1085503.2706762394\n",
            "Epoch 95/100: Loss: 1085502.2479876713\n",
            "Epoch 96/100: Loss: 1085501.2264975817\n",
            "Epoch 97/100: Loss: 1085500.2062059592\n",
            "Epoch 98/100: Loss: 1085499.1871127933\n",
            "Epoch 99/100: Loss: 1085498.1692180731\n",
            "The loss function decreases but the loss is too high and will take too much time to train ! , will overcome this later vy using Adagrad optimizer \n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "\n",
        "for i in range(100):\n",
        "    # Iterate through training dataloader\n",
        "    #print(X)\n",
        "   \n",
        "    # Generate Prediction\n",
        "    preds = model(X,theta)\n",
        "    loss = loss_fn(preds,rat_tensor_mean,X,theta)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        X -= X.grad *1e-7\n",
        "        theta -= theta.grad * 1e-7\n",
        "        # Set the gradients to zero\n",
        "        X.grad.zero_()\n",
        "        theta.grad.zero_()\n",
        "\n",
        "    print(f\"Epoch {i}/{epochs}: Loss: {loss}\")\n",
        "\n",
        "print(\"The loss function decreases but the loss is too high and will take too much time to train ! , will overcome this later vy using Adagrad optimizer \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZiaDd9HK4RW",
        "outputId": "dc39e3e4-f40f-4940-d568-75d00c6a7ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2.5680e+00,  3.3531e+00, -1.1466e+00,  ...,  4.1232e-01,\n",
            "          3.3527e+00, -5.3684e+00],\n",
            "        [-6.6235e-01,  3.4675e-01,  2.1031e-01,  ..., -1.4643e-01,\n",
            "          2.9233e+00,  3.4158e+00],\n",
            "        [ 9.2118e-01,  1.3144e+00, -9.3630e-01,  ..., -6.1703e+00,\n",
            "          3.4750e+00, -2.1345e+00],\n",
            "        ...,\n",
            "        [ 1.6614e+00,  2.8331e+00, -2.1030e+00,  ..., -1.1588e-01,\n",
            "          2.1031e+00, -3.3417e+00],\n",
            "        [ 5.2432e-01, -4.8543e+00,  1.8346e+00,  ...,  5.6003e+00,\n",
            "         -4.2558e+00,  2.7036e+00],\n",
            "        [ 1.7781e+00,  1.2427e-03, -9.4708e-01,  ...,  6.2157e+00,\n",
            "          2.0036e+00, -2.8159e+00]], dtype=torch.float64,\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(X @ theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYENEMK2OkLx",
        "outputId": "26112340-e60b-46c4-e1a8-5e37cf6f6b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4.  0.  4.  ... 0.  0.  0. ]\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " ...\n",
            " [2.5 2.  2.  ... 0.  0.  0. ]\n",
            " [3.  0.  0.  ... 0.  0.  0. ]\n",
            " [5.  0.  0.  ... 0.  0.  0. ]]\n"
          ]
        }
      ],
      "source": [
        "print(Rat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZNyggTkOztF",
        "outputId": "6929dd37-0d9d-4ccd-80ac-ee0e35d7996d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1085604.7562, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(loss_fn((X @theta),Rat,X,theta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmyLcie_O7Tq",
        "outputId": "6177e24a-6799-4421-dff1-28ce60641fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4.  0.  4.  ... 0.  0.  0. ]\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " ...\n",
            " [2.5 2.  2.  ... 0.  0.  0. ]\n",
            " [3.  0.  0.  ... 0.  0.  0. ]\n",
            " [5.  0.  0.  ... 0.  0.  0. ]]\n",
            "tensor([[-0.7944, -0.6532,  0.3844,  ..., -0.7768, -1.1721, -1.2436],\n",
            "        [-1.3465, -1.9497,  0.6739,  ...,  0.4272, -0.2464,  1.9913],\n",
            "        [ 1.5645, -0.3043, -1.0690,  ..., -2.1173,  0.1139,  0.0481],\n",
            "        ...,\n",
            "        [ 0.0800, -0.2370, -0.8331,  ...,  0.0671,  0.8576,  0.5397],\n",
            "        [-0.5923,  0.6851,  0.3072,  ..., -0.3841,  0.2433, -0.3813],\n",
            "        [-0.0268, -0.1760,  0.0970,  ..., -2.6719,  0.6093, -0.1754]],\n",
            "       dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(Rat)\n",
        "print(theta)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CF_algorithm.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0951120135be19c03de18811d16e6df95fbcb893a79b308f148af7953a19ee91"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('Engage_proj')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
