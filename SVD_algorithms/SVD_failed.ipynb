{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "GGqQp7uJVzwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        userId  movieId  rating   timestamp\n",
            "0            1        1     4.0   964982703\n",
            "1            1        3     4.0   964981247\n",
            "2            1        6     4.0   964982224\n",
            "3            1       47     5.0   964983815\n",
            "4            1       50     5.0   964982931\n",
            "...        ...      ...     ...         ...\n",
            "100831     610   166534     4.0  1493848402\n",
            "100832     610   168248     5.0  1493850091\n",
            "100833     610   168250     5.0  1494273047\n",
            "100834     610   168252     5.0  1493846352\n",
            "100835     610   170875     3.0  1493846415\n",
            "\n",
            "[100836 rows x 4 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n",
            "(100837, 4)\n",
            "[[1.00000e+00 1.00000e+00 4.00000e+00]\n",
            " [1.00000e+00 3.00000e+00 4.00000e+00]\n",
            " [1.00000e+00 6.00000e+00 4.00000e+00]\n",
            " ...\n",
            " [6.10000e+02 1.68250e+05 5.00000e+00]\n",
            " [6.10000e+02 1.68252e+05 5.00000e+00]\n",
            " [6.10000e+02 1.70875e+05 3.00000e+00]]\n",
            "9742\n",
            "9742\n",
            "<class 'dict'>\n",
            "(9742,)\n",
            "201\n",
            "(array([   0,    2,    5,   43,   46,   62,   89,   97,  124,  130,  136,\n",
            "        184,  190,  197,  201,  224,  257,  275,  291,  307,  314,  320,\n",
            "        325,  367,  384,  398,  418,  436,  461,  476,  484,  485,  508,\n",
            "        509,  510,  513,  520,  546,  551,  559,  592,  594,  615,  632,\n",
            "        701,  705,  720,  723,  734,  767,  781,  782,  783,  786,  787,\n",
            "        788,  789,  797,  801,  810,  815,  820,  828,  829,  831,  836,\n",
            "        856,  863,  898,  899,  900,  907,  909,  911,  914,  915,  920,\n",
            "        921,  923,  925,  927,  939,  955,  957,  964,  969,  974,  977,\n",
            "        981,  990,  996, 1036, 1060, 1076, 1084, 1110, 1126, 1146, 1154,\n",
            "       1171, 1181, 1183, 1190, 1218, 1220, 1224, 1235, 1261, 1298, 1319,\n",
            "       1326, 1332, 1333, 1401, 1407, 1431, 1444, 1475, 1480, 1487, 1493,\n",
            "       1503, 1505, 1516, 1517, 1522, 1526, 1543, 1553, 1556, 1557, 1559,\n",
            "       1562, 1567, 1576, 1577, 1595, 1597, 1599, 1601, 1617, 1628, 1644,\n",
            "       1674, 1687, 1691, 1704, 1734, 1743, 1755, 1768, 1788, 1790, 1796,\n",
            "       1806, 1814, 1826, 1842, 1850, 1858, 1866, 1874, 1883, 1904, 1905,\n",
            "       1917, 1939, 1946, 1957, 1971, 1972, 1979, 1986, 1987, 1990, 1991,\n",
            "       1994, 1997, 2020, 2028, 2038, 2077, 2103, 2126, 2145, 2157, 2182,\n",
            "       2193, 2216, 2218, 2219, 2220, 2226, 2248, 2250, 2254, 2256, 2259,\n",
            "       2286, 2287, 2302, 2303, 2310, 2372, 2388, 2394, 2431, 2434, 2440,\n",
            "       2460, 2526, 2571, 2572, 2573, 2579, 2581, 2603, 2608, 2636, 2674,\n",
            "       2696, 2713, 2733, 2764, 2765, 2788, 2798, 2802, 2836, 2847, 2991,\n",
            "       3673]),)\n",
            "(611, 9742)\n",
            "(610, 9742)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "%run dataloader.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9742, 1)\n",
            "shape of rat_bool =  (9742, 610)\n",
            "shape of Rat =  (610, 9742)\n",
            "num_users = 610\n",
            "rat_tensor shape =  torch.Size([9742, 610])\n",
            "rat_tensor_masked shape =  torch.Size([9742, 610])\n",
            "sum shape =  torch.Size([9742, 1])\n",
            "tensor([[ 0.0791, -3.9209, -3.9209,  ..., -1.4209, -0.9209,  1.0791],\n",
            "        [-3.4318, -3.4318, -3.4318,  ..., -1.4318, -3.4318, -3.4318],\n",
            "        [ 0.7404, -3.2596, -3.2596,  ..., -1.2596, -3.2596, -3.2596],\n",
            "        ...,\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-4.0000, -4.0000, -4.0000,  ..., -4.0000, -4.0000, -4.0000]],\n",
            "       dtype=torch.float64)\n",
            "torch.Size([9742, 610])\n"
          ]
        }
      ],
      "source": [
        "Rat = final_ratings\n",
        "rat_bool = np.array(final_ratings.T > 0)\n",
        "nums = np.sum(rat_bool,axis = 1,keepdims=True)\n",
        "print(nums.shape)\n",
        "nums_tensor = torch.from_numpy(nums)\n",
        "print(\"shape of rat_bool = \",rat_bool.shape)\n",
        "rat_bool_tensor = torch.from_numpy(rat_bool)\n",
        "print(\"shape of Rat = \",Rat.shape)\n",
        "num_users = Rat.shape[0]\n",
        "print(\"num_users =\",num_users)\n",
        "num_movies = Rat.shape[1]\n",
        "num_features = 10\n",
        "rat_tensor = torch.from_numpy(Rat.T)\n",
        "print(\"rat_tensor shape = \",rat_tensor.shape)\n",
        "rat_tensor_masked = torch.mul(rat_tensor,rat_bool_tensor)\n",
        "print(\"rat_tensor_masked shape = \",rat_tensor_masked.shape)\n",
        "print(\"sum shape = \",torch.sum(rat_tensor_masked,dim=1,keepdim=True).shape)\n",
        "rat_tensor_mean = rat_tensor_masked - (torch.div( (torch.sum(rat_tensor_masked,dim=1,keepdim=True)) ,nums_tensor))\n",
        "print(rat_tensor_mean)\n",
        "X = torch.randn((num_movies,10),dtype=float,requires_grad = True)\n",
        "theta = torch.randn((num_features,num_users),dtype=float,requires_grad=True)\n",
        "preds = X @ theta\n",
        "\n",
        "print(rat_bool_tensor.shape)\n",
        "# diff = preds-rat_tensor\n",
        "# print(diff.shape)\n",
        "# diff_select = torch.masked_select(diff,rat_bool_tensor)\n",
        "# mse_select = (torch.square(diff_select)).sum\n",
        "# weight_decay = 1e-5\n",
        "# regex = (torch.sum(torch.square(X)))*weight_decay\n",
        "# regtheta = (torch.sum(torch.square(theta)))*weight_decay\n",
        "# loss = (torch.square(torch.masked_select(rat_tensor_mean - (X @ theta),rat_bool_tensor))).sum() + regex + regtheta\n",
        "# print(loss)\n",
        "# loss.backward()\n",
        "# print(X.grad)\n",
        "# print(theta.grad)\n",
        "# X = X - (1e-3) * X.grad\n",
        "# theta = theta - (1e-3) * theta.grad\n",
        "# print(type(theta))\n",
        "# loss = (torch.square(torch.masked_select(rat_tensor_mean - (X @ theta),rat_bool_tensor))).sum() + regex + regtheta\n",
        "# print(loss)\n",
        "\n",
        "# X = torch.randn((6,4),dtype = float,requires_grad = True)\n",
        "# theta = torch.randn((4,3),dtype =float, requires_grad = True)\n",
        "# preds = X @ theta \n",
        "# loss_fn = F.mse_loss\n",
        "\n",
        "# print(regex)\n",
        "# loss = loss_fn((X @ theta), Rat) + regex + regtheta\n",
        "# print(\"loss = \",loss) # use loss.item()\n",
        "# print(X,\"\\n\",theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSjCagd7V055",
        "outputId": "4fe11c19-b468-489a-e8f6-a22082299e28"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Rat = np.array([[4,5,6,7,8,9],[3,4,6,7,1,9],[2,5,6,8,9,10]],dtype=float).T\n",
        "# Rat = torch.from_numpy(Rat)\n",
        "# print(Rat) # 5,3\n",
        "# X = torch.randn((6,4),dtype = float,requires_grad = True)\n",
        "# theta = torch.randn((4,3),dtype =float, requires_grad = True)\n",
        "# preds = X @ theta \n",
        "# loss_fn = F.mse_loss\n",
        "# weight_decay = 1e-5\n",
        "# regex = (torch.sum(torch.square(X)))*weight_decay\n",
        "# regtheta = (torch.sum(torch.square(theta)))*weight_decay\n",
        "# print(regex)\n",
        "# loss = loss_fn((X @ theta), Rat) + regex + regtheta\n",
        "# print(\"loss = \",loss) # use loss.item()\n",
        "# print(X,\"\\n\",theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2gP3-NWbwmD",
        "outputId": "2d8272ce-4d24-4b86-e7db-75b5a93d7c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[ 0.0791, -3.9209, -3.9209,  ..., -1.4209, -0.9209,  1.0791],\n",
            "        [-3.4318, -3.4318, -3.4318,  ..., -1.4318, -3.4318, -3.4318],\n",
            "        [ 0.7404, -3.2596, -3.2596,  ..., -1.2596, -3.2596, -3.2596],\n",
            "        ...,\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-3.5000, -3.5000, -3.5000,  ..., -3.5000, -3.5000, -3.5000],\n",
            "        [-4.0000, -4.0000, -4.0000,  ..., -4.0000, -4.0000, -4.0000]],\n",
            "       dtype=torch.float64),)\n",
            "[tensor([[ 0.0791, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,  0.5791, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -1.4209, -3.9209,\n",
            "          0.5791, -0.4209,  0.0791, -3.9209, -0.4209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -0.9209, -3.9209, -3.9209, -3.9209,  1.0791, -0.9209,\n",
            "         -0.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  1.0791, -0.9209,  0.0791,  1.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209, -3.9209, -3.9209, -0.9209, -3.9209, -3.9209,\n",
            "          1.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,  0.0791,\n",
            "         -3.9209,  0.0791, -3.9209, -1.4209, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "          0.5791, -3.9209, -3.9209, -3.4209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -1.4209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -0.9209, -0.9209,  0.0791, -3.9209, -0.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209,  0.5791, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209, -0.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -0.4209, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209, -1.9209, -3.9209, -0.9209,  0.0791, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209, -0.9209,  0.0791, -3.9209, -3.9209, -0.4209,\n",
            "          1.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "         -1.9209, -3.9209, -0.9209,  0.0791, -3.9209, -3.9209,  0.5791,  0.0791,\n",
            "          0.0791, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791, -0.4209, -3.9209,\n",
            "          0.5791, -3.9209,  1.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "          1.0791,  0.0791,  0.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "          0.0791,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,\n",
            "         -1.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -0.4209,\n",
            "          1.0791,  0.0791, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -0.4209, -0.9209, -3.9209, -0.9209,\n",
            "          0.0791, -3.9209, -0.4209,  1.0791, -3.9209, -3.9209, -0.4209, -3.9209,\n",
            "         -3.9209, -0.4209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209, -0.4209,\n",
            "         -0.9209,  1.0791, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791,  1.0791,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209,  0.5791, -3.9209,  0.5791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791,  0.0791,\n",
            "         -3.9209, -1.9209, -3.9209, -3.9209,  1.0791,  1.0791, -3.9209, -3.9209,\n",
            "          1.0791,  0.0791,  1.0791,  0.0791,  0.0791, -3.9209, -0.9209,  0.5791,\n",
            "         -3.9209,  0.5791, -0.9209, -3.9209, -3.9209, -3.9209, -3.9209,  0.5791,\n",
            "         -3.9209,  0.0791,  0.0791,  0.0791, -0.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -1.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.4209, -0.4209, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791, -3.9209, -0.4209, -3.9209,  0.0791,\n",
            "          0.0791, -3.9209,  0.0791, -3.9209,  1.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "          1.0791, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209,  0.0791, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  1.0791, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -0.9209, -0.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209,  0.5791, -3.9209,  1.0791, -0.4209,  0.5791, -3.9209, -3.9209,\n",
            "          0.0791, -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -0.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  0.0791, -3.9209,\n",
            "         -0.4209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  1.0791, -1.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -3.9209, -1.4209,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791, -3.9209,  0.5791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  1.0791, -3.9209,  1.0791, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209, -3.9209,  0.5791, -3.9209, -2.4209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791,  0.0791,  0.0791,  1.0791, -3.9209,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791,  0.0791, -3.9209, -3.9209, -0.9209,\n",
            "         -3.9209, -3.9209,  0.0791,  0.5791, -3.9209, -3.9209, -3.9209,  0.5791,\n",
            "         -3.9209, -0.4209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,  0.0791,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209,  0.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -0.9209, -3.9209,  0.0791,  0.0791, -3.9209, -3.9209, -1.4209,\n",
            "         -0.9209, -3.9209, -3.9209, -3.9209,  1.0791,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -0.9209, -3.9209, -3.9209, -0.9209,\n",
            "         -3.9209, -3.9209, -3.9209, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -3.9209, -3.9209, -3.9209,  1.0791, -0.9209,\n",
            "          0.0791,  0.5791, -3.9209, -3.9209, -3.9209, -3.9209, -0.4209, -3.9209,\n",
            "         -3.9209,  0.0791, -3.9209,  0.0791,  1.0791, -3.9209, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209,  0.0791, -0.9209, -3.9209, -3.9209, -3.9209,  1.0791,\n",
            "         -3.9209, -3.9209,  1.0791, -3.9209, -3.9209,  0.0791, -3.9209, -3.9209,\n",
            "         -3.9209, -3.9209, -3.9209,  0.0791,  0.0791, -3.9209, -0.9209, -1.4209,\n",
            "          0.0791, -3.9209,  0.0791, -0.9209,  0.0791, -1.4209,  0.0791, -1.4209,\n",
            "         -0.9209,  1.0791]], dtype=torch.float64)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_ds = TensorDataset(rat_tensor_mean)\n",
        "print(train_ds.tensors)\n",
        "train_dl = DataLoader(train_ds,shuffle = False)\n",
        "for rat in train_dl: \n",
        "    print(rat)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model(X,theta):\n",
        "    return X @ theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(preds,targets,X,theta):\n",
        "    weight_decay = 1e-5\n",
        "    regex = (torch.sum(torch.square(X)))*weight_decay\n",
        "    regtheta = (torch.sum(torch.square(theta)))*weight_decay\n",
        "    return  (torch.square(torch.masked_select(rat_tensor_mean - (X @ theta),rat_bool_tensor))).sum() + regex + regtheta\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1049963.1244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor([-1548.7674, -1715.9091, -1818.8654,  ..., -2131.5000, -2131.5000,\n",
            "        -2436.0000], dtype=torch.float64)\n",
            "tensor([[-0.3885, -0.9913,  3.1413,  ...,  0.1907, -2.7712, -0.5394],\n",
            "        [ 2.9572, -0.8047, -0.0039,  ..., -0.0868,  1.3606,  0.6509],\n",
            "        [ 1.2241,  0.7639, -0.0064,  ..., -1.4547, -0.7896, -0.7220],\n",
            "        ...,\n",
            "        [-1.2248, -0.4652,  1.7068,  ...,  0.1482,  1.2827, -0.9495],\n",
            "        [ 0.0319,  1.3526, -0.8578,  ...,  0.4516, -0.6249, -0.3000],\n",
            "        [ 0.7485,  1.7176, -1.0088,  ..., -2.2162, -0.2102,  0.9666]],\n",
            "       dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "preds = model(X,theta)\n",
        "print(loss_fn(preds,rat_tensor_mean,X,theta))\n",
        "print(torch.sum(rat_tensor_mean,dim = 1))\n",
        "loss = (loss_fn(preds,rat_tensor_mean,X,theta))\n",
        "loss.backward()\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/100: Loss: 1049963.1244317894\n",
            "Epoch 1/100: Loss: 1049967.6270006846\n",
            "Epoch 2/100: Loss: 1049969.8801740524\n",
            "Epoch 3/100: Loss: 1049972.134426143\n",
            "Epoch 4/100: Loss: 1049974.3897569631\n",
            "Epoch 5/100: Loss: 1049976.6461665214\n",
            "Epoch 6/100: Loss: 1049978.9036548256\n",
            "Epoch 7/100: Loss: 1049981.162221883\n",
            "Epoch 8/100: Loss: 1049983.4218677015\n",
            "Epoch 9/100: Loss: 1049985.682592289\n",
            "Epoch 10/100: Loss: 1049987.9443956534\n",
            "Epoch 11/100: Loss: 1049990.2072778023\n",
            "Epoch 12/100: Loss: 1049992.4712387433\n",
            "Epoch 13/100: Loss: 1049994.7362784848\n",
            "Epoch 14/100: Loss: 1049997.0023970343\n",
            "Epoch 15/100: Loss: 1049999.2695943993\n",
            "Epoch 16/100: Loss: 1050001.5378705878\n",
            "Epoch 17/100: Loss: 1050003.8072256078\n",
            "Epoch 18/100: Loss: 1050006.077659467\n",
            "Epoch 19/100: Loss: 1050008.3491721738\n",
            "Epoch 20/100: Loss: 1050010.6217637348\n",
            "Epoch 21/100: Loss: 1050012.8954341586\n",
            "Epoch 22/100: Loss: 1050015.1701834532\n",
            "Epoch 23/100: Loss: 1050017.4460116262\n",
            "Epoch 24/100: Loss: 1050019.7229186855\n",
            "Epoch 25/100: Loss: 1050022.0009046388\n",
            "Epoch 26/100: Loss: 1050024.279969494\n",
            "Epoch 27/100: Loss: 1050026.5601132591\n",
            "Epoch 28/100: Loss: 1050028.841335942\n",
            "Epoch 29/100: Loss: 1050031.1236375505\n",
            "Epoch 30/100: Loss: 1050033.4070180925\n",
            "Epoch 31/100: Loss: 1050035.6914775758\n",
            "Epoch 32/100: Loss: 1050037.9770160085\n",
            "Epoch 33/100: Loss: 1050040.263633398\n",
            "Epoch 34/100: Loss: 1050042.551329753\n",
            "Epoch 35/100: Loss: 1050044.8401050807\n",
            "Epoch 36/100: Loss: 1050047.1299593893\n",
            "Epoch 37/100: Loss: 1050049.4208926866\n",
            "Epoch 38/100: Loss: 1050051.7129049809\n",
            "Epoch 39/100: Loss: 1050054.0059962792\n",
            "Epoch 40/100: Loss: 1050056.3001665906\n",
            "Epoch 41/100: Loss: 1050058.5954159223\n",
            "Epoch 42/100: Loss: 1050060.8917442823\n",
            "Epoch 43/100: Loss: 1050063.189151679\n",
            "Epoch 44/100: Loss: 1050065.4876381198\n",
            "Epoch 45/100: Loss: 1050067.7872036125\n",
            "Epoch 46/100: Loss: 1050070.0878481658\n",
            "Epoch 47/100: Loss: 1050072.389571787\n",
            "Epoch 48/100: Loss: 1050074.6923744844\n",
            "Epoch 49/100: Loss: 1050076.996256266\n",
            "Epoch 50/100: Loss: 1050079.3012171395\n",
            "Epoch 51/100: Loss: 1050081.6072571133\n",
            "Epoch 52/100: Loss: 1050083.9143761953\n",
            "Epoch 53/100: Loss: 1050086.2225743926\n",
            "Epoch 54/100: Loss: 1050088.5318517145\n",
            "Epoch 55/100: Loss: 1050090.8422081685\n",
            "Epoch 56/100: Loss: 1050093.1536437622\n",
            "Epoch 57/100: Loss: 1050095.466158504\n",
            "Epoch 58/100: Loss: 1050097.7797524019\n",
            "Epoch 59/100: Loss: 1050100.0944254636\n",
            "Epoch 60/100: Loss: 1050102.4101776974\n",
            "Epoch 61/100: Loss: 1050104.7270091113\n",
            "Epoch 62/100: Loss: 1050107.0449197136\n",
            "Epoch 63/100: Loss: 1050109.3639095118\n",
            "Epoch 64/100: Loss: 1050111.6839785143\n",
            "Epoch 65/100: Loss: 1050114.005126729\n",
            "Epoch 66/100: Loss: 1050116.327354164\n",
            "Epoch 67/100: Loss: 1050118.650660827\n",
            "Epoch 68/100: Loss: 1050120.975046727\n",
            "Epoch 69/100: Loss: 1050123.3005118712\n",
            "Epoch 70/100: Loss: 1050125.627056268\n",
            "Epoch 71/100: Loss: 1050127.9546799255\n",
            "Epoch 72/100: Loss: 1050130.2833828514\n",
            "Epoch 73/100: Loss: 1050132.613165054\n",
            "Epoch 74/100: Loss: 1050134.9440265417\n",
            "Epoch 75/100: Loss: 1050137.2759673225\n",
            "Epoch 76/100: Loss: 1050139.608987404\n",
            "Epoch 77/100: Loss: 1050141.9430867946\n",
            "Epoch 78/100: Loss: 1050144.2782655025\n",
            "Epoch 79/100: Loss: 1050146.6145235358\n",
            "Epoch 80/100: Loss: 1050148.9518609028\n",
            "Epoch 81/100: Loss: 1050151.290277611\n",
            "Epoch 82/100: Loss: 1050153.629773669\n",
            "Epoch 83/100: Loss: 1050155.970349085\n",
            "Epoch 84/100: Loss: 1050158.312003867\n",
            "Epoch 85/100: Loss: 1050160.654738023\n",
            "Epoch 86/100: Loss: 1050162.998551561\n",
            "Epoch 87/100: Loss: 1050165.3434444896\n",
            "Epoch 88/100: Loss: 1050167.689416817\n",
            "Epoch 89/100: Loss: 1050170.0364685506\n",
            "Epoch 90/100: Loss: 1050172.3845996996\n",
            "Epoch 91/100: Loss: 1050174.7338102716\n",
            "Epoch 92/100: Loss: 1050177.0841002746\n",
            "Epoch 93/100: Loss: 1050179.4354697166\n",
            "Epoch 94/100: Loss: 1050181.7879186065\n",
            "Epoch 95/100: Loss: 1050184.1414469518\n",
            "Epoch 96/100: Loss: 1050186.4960547616\n",
            "Epoch 97/100: Loss: 1050188.8517420432\n",
            "Epoch 98/100: Loss: 1050191.2085088051\n",
            "Epoch 99/100: Loss: 1050193.5663550557\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "\n",
        "for i in range(100):\n",
        "    # Iterate through training dataloader\n",
        "    #print(X)\n",
        "   \n",
        "    # Generate Prediction\n",
        "    preds = model(X,theta)\n",
        "    loss = loss_fn(preds,rat_tensor_mean,X,theta)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        X -= X.grad *1e-7\n",
        "        theta -= theta.grad * 1e-7\n",
        "        # Set the gradients to zero\n",
        "        X.grad.zero_()\n",
        "        theta.grad.zero_()\n",
        "\n",
        "    print(f\"Epoch {i}/{epochs}: Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "4C3dDl2vV1Lu"
      },
      "outputs": [],
      "source": [
        "# opt = torch.optim.SGD((X,theta), lr = 1e-3, weight_decay = 1e-5)\n",
        "# opt.step()\n",
        "# num_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "I5wM9nhmZXdt"
      },
      "outputs": [],
      "source": [
        "# for epoch in range(num_epochs):\n",
        "        \n",
        "#         # Train with batches of data\n",
        "#         for rat in train_dl:\n",
        "            \n",
        "#             # 1. Generate predictions\n",
        "#             preds = X @ theta\n",
        "            \n",
        "#             # 2. Calculate loss\n",
        "            \n",
        "\n",
        "#             print(loss)\n",
        "            \n",
        "#             # 3. Compute gradients\n",
        "#             loss.backward()\n",
        "            \n",
        "#             # 4. Update parameters using gradients\n",
        "#             opt.step()\n",
        "            \n",
        "#             # 5. Reset the gradients to zero\n",
        "#             opt.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh0O5Cj6D2Fn",
        "outputId": "822f5f1a-9882-45a4-ae6b-64a5f98afa56"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# rat = Rat.clone().detach()\n",
        "# print(rat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "mHI153ghaDBh"
      },
      "outputs": [],
      "source": [
        "# num_epochs = 10000\n",
        "# learning_rate  = 1e-3\n",
        "# for i in range (num_epochs):\n",
        "#   preds = X @ theta\n",
        "#   loss = (torch.square(torch.masked_select(rat_tensor_mean - (X @ theta),rat_bool_tensor))).sum() + regex + regtheta\n",
        "#   loss.backward(retain_graph=True)\n",
        "#   with torch.no_grad():\n",
        "#     theta -= theta.grad*learning_rate\n",
        "#     X -= X.grad*learning_rate\n",
        "#     X.grad.zero_()\n",
        "#     theta.grad.zero_()\n",
        "#   #the variables in this system are X, theta and rat ( ratings ) \n",
        "  \n",
        "# print(loss_fn((X @ theta),Rat), '\\n')\n",
        "# print(\"Predictions = \\n\", (X @ theta))\n",
        "# print(\"Ratings =\\n \",Rat)\n",
        "# print(X)\n",
        "# print(theta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "N9LxCdyXd9hh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1050738.5493, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050741.1459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050743.7436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050746.3424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050748.9422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050751.5432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050754.1452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050756.7483, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050759.3525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050761.9578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050764.5641, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050767.1716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050769.7801, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050772.3897, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050775.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050777.6121, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050780.2250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050782.8389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050785.4539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050788.0700, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050790.6871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050793.3054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050795.9247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050798.5451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050801.1666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050803.7892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050806.4129, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050809.0376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050811.6634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050814.2903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050816.9183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050819.5474, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050822.1775, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050824.8087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050827.4411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050830.0744, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050832.7089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050835.3445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050837.9811, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050840.6188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050843.2576, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050845.8975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050848.5385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050851.1805, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050853.8237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050856.4679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050859.1132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050861.7595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "tensor(1050864.4070, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/arkartik/engage_2022/Source_code/CF_algorithm_dataset.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arkartik/engage_2022/Source_code/CF_algorithm_dataset.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m train_dl:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arkartik/engage_2022/Source_code/CF_algorithm_dataset.ipynb#ch0000007?line=3'>4</a>\u001b[0m   loss \u001b[39m=\u001b[39m loss_fn(X \u001b[39m@\u001b[39m theta, rat_tensor_mean,X,theta)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arkartik/engage_2022/Source_code/CF_algorithm_dataset.ipynb#ch0000007?line=4'>5</a>\u001b[0m   loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arkartik/engage_2022/Source_code/CF_algorithm_dataset.ipynb#ch0000007?line=5'>6</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arkartik/engage_2022/Source_code/CF_algorithm_dataset.ipynb#ch0000007?line=6'>7</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/arkartik/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "optimizer = torch.optim.SGD((X,theta), lr = 1e-7 , weight_decay = 1e-5)\n",
        "for i in range (3):\n",
        "  for r in train_dl:\n",
        "    loss = loss_fn(X @ theta, rat_tensor_mean,X,theta)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZiaDd9HK4RW",
        "outputId": "dc39e3e4-f40f-4940-d568-75d00c6a7ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3.9998, 2.9998, 2.0002],\n",
            "        [5.0002, 4.0003, 4.9995],\n",
            "        [5.9998, 5.9998, 6.0002],\n",
            "        [6.9997, 6.9996, 8.0004],\n",
            "        [7.9999, 1.0000, 9.0000],\n",
            "        [9.0002, 9.0003, 9.9996]], dtype=torch.float64, grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(X @ theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYENEMK2OkLx",
        "outputId": "26112340-e60b-46c4-e1a8-5e37cf6f6b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4.,  3.,  2.],\n",
            "        [ 5.,  4.,  5.],\n",
            "        [ 6.,  6.,  6.],\n",
            "        [ 7.,  7.,  8.],\n",
            "        [ 8.,  1.,  9.],\n",
            "        [ 9.,  9., 10.]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "print(Rat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZNyggTkOztF",
        "outputId": "6929dd37-0d9d-4ccd-80ac-ee0e35d7996d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.3555e-08, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(loss_fn((X @theta),Rat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmyLcie_O7Tq",
        "outputId": "6177e24a-6799-4421-dff1-28ce60641fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4.,  3.,  2.],\n",
            "        [ 5.,  4.,  5.],\n",
            "        [ 6.,  6.,  6.],\n",
            "        [ 7.,  7.,  8.],\n",
            "        [ 8.,  1.,  9.],\n",
            "        [ 9.,  9., 10.]], dtype=torch.float64)\n",
            "tensor([[-1.3772, -1.2660,  0.6727],\n",
            "        [ 2.9676,  1.6857,  3.2378],\n",
            "        [-1.3171,  0.8915, -1.0399],\n",
            "        [ 0.5296, -1.8645, -1.6353]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(Rat)\n",
        "print(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhxE3IYRSrBW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CF_algorithm.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8201e9342a4a492ddbd4e81efec90b2ccf0d205cda2cc39ac893f0c43374b5e0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
